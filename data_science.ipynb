{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR46yAcT8AE+ZT6bt2LHwS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharaneesh-EM/Dharaneesh-EM/blob/main/data_science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to collect data on housing prices for a real estate project.Dataset: For this question, you won't provide a specific dataset. Instead, focus on the process of acquiring data. Question: Describe a plan to acquire the necessary data from at least three different sources (e.g., web scraping real estate websites, using public APIs for property data, accessing government datasets on property values). Outline the steps you would take to gather the data, handle any inconsistencies or missing values, and combine it into a usable format for analysis. Discuss the challenges and limitations of each data source."
      ],
      "metadata": {
        "id": "-3MZCOvTja5p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sySKtBUwjhmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Simulated data from different sources (Web Scraping, APIs, Government Data)\n",
        "data_zillow = pd.DataFrame({\n",
        "    'Property_ID': [101, 104],\n",
        "    'Address': ['123 Main St', '1011 Maple Ln'],\n",
        "    'City': ['New York', 'Miami'],\n",
        "    'Price': [550000, 680000],\n",
        "    'Sq_Ft': [1200, 2000],\n",
        "    'Bedrooms': [3, 5],\n",
        "    'Source': ['Zillow', 'Zillow'],\n",
        "    'Date_Updated': ['2024-02-01', '2024-02-03']\n",
        "})\n",
        "\n",
        "data_realtor = pd.DataFrame({\n",
        "    'Property_ID': [103],\n",
        "    'Address': ['789 Pine Dr'],\n",
        "    'City': ['Houston'],\n",
        "    'Price': [300000],\n",
        "    'Sq_Ft': [1800],\n",
        "    'Bedrooms': [3],\n",
        "    'Source': ['Realtor'],\n",
        "    'Date_Updated': ['2024-02-05']\n",
        "})\n",
        "\n",
        "data_gov = pd.DataFrame({\n",
        "    'Property_ID': [102, 105],\n",
        "    'Address': ['456 Oak Ave', '1213 Cedar Rd'],\n",
        "    'City': ['Chicago', 'Seattle'],\n",
        "    'Price': [420000, 490000],\n",
        "    'Sq_Ft': [1500, 1400],\n",
        "    'Bedrooms': [4, 3],\n",
        "    'Source': ['Govt API', 'Govt API'],\n",
        "    'Date_Updated': ['2024-01-20', '2024-01-25']\n",
        "})\n",
        "\n",
        "# Convert Date_Updated to datetime format for proper merging\n",
        "for df in [data_zillow, data_realtor, data_gov]:\n",
        "    df['Date_Updated'] = pd.to_datetime(df['Date_Updated'])\n",
        "\n",
        "# Merge all data sources into a single DataFrame\n",
        "merged_data = pd.concat([data_zillow, data_realtor, data_gov], ignore_index=True)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "merged_data.fillna({'Price': merged_data['Price'].median(), 'Sq_Ft': merged_data['Sq_Ft'].median()}, inplace=True)\n",
        "\n",
        "# Sort by Date_Updated to keep the latest data\n",
        "merged_data.sort_values(by='Date_Updated', ascending=False, inplace=True)\n",
        "\n",
        "# Reset index after sorting\n",
        "merged_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display final cleaned dataset\n",
        "print(merged_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o38ffI1aZgB7",
        "outputId": "b68c30c4-ea8a-4d3e-febc-ad7f43f0b560"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Property_ID        Address      City   Price  Sq_Ft  Bedrooms    Source  \\\n",
            "0          103    789 Pine Dr   Houston  300000   1800         3   Realtor   \n",
            "1          104  1011 Maple Ln     Miami  680000   2000         5    Zillow   \n",
            "2          101    123 Main St  New York  550000   1200         3    Zillow   \n",
            "3          105  1213 Cedar Rd   Seattle  490000   1400         3  Govt API   \n",
            "4          102    456 Oak Ave   Chicago  420000   1500         4  Govt API   \n",
            "\n",
            "  Date_Updated  \n",
            "0   2024-02-05  \n",
            "1   2024-02-03  \n",
            "2   2024-02-01  \n",
            "3   2024-01-25  \n",
            "4   2024-01-20  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To collect housing price data, we can use three main sources:\n",
        "\n",
        "Web Scraping Real Estate Websites\n",
        "\n",
        "Extracts real-time listings from sites like Zillow or Realtor.com.\n",
        "Challenges: Legal restrictions, IP blocking, and missing data.\n",
        "Public APIs for Property Data\n",
        "\n",
        "APIs from Zillow, Redfin, or government portals provide structured data.\n",
        "Challenges: Limited free access, restricted data fields, and format differences.\n",
        "Government & Open Data Portals\n",
        "\n",
        "Official records on property values, taxes, and sales history.\n",
        "Challenges: Delayed updates, inconsistent formats, and accessibility issues.\n",
        "Steps to Clean & Use Data:\n",
        "Remove duplicates, fill missing values, and standardize formats.\n",
        "Merge datasets using common property details.\n",
        "Analyze trends, price fluctuations, and investment opportunities."
      ],
      "metadata": {
        "id": "x6XWNwvQjopw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's generate more scenario-based, program-based questions at Bloom's Taxonomy Levels 3 (Applying) and 4 (Analyzing) for the topics you provided, including datasets or dataset generation instructions.\n",
        "NumPy - Applying (Level 3) Scenario: You're working with audio data represented as a 1D NumPy array. You need to apply a fade-in effect to the beginning of the audio.\n",
        "\n",
        "Reasoning: This task requires applying a linear fade-in effect over a specified period and then visualizing the impact on the audio signal, testing your understanding of signal manipulation and NumPy arrays."
      ],
      "metadata": {
        "id": "FZNAqBiKjv1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  # For visualization (optional)\n",
        "\n",
        "# Create sample audio data (a sine wave for demonstration)\n",
        "sample_rate = 44100  # Samples per second\n",
        "duration = 5  # Seconds\n",
        "time = np.linspace(0, duration, int(sample_rate * duration), False)\n",
        "audio_data = np.sin(2 * np.pi * 440 * time)  # 440 Hz sine wave\n"
      ],
      "metadata": {
        "id": "2qHKm_6Gj5rB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy - Analyzing (Level 4) Scenario: You are analyzing time-series data representing stock prices. You need to calculate the moving average of the stock prices to identify trends."
      ],
      "metadata": {
        "id": "dtp75Catj96U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create sample stock price data (replace with your actual data)\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range('2023-01-01', periods=100)\n",
        "stock_prices = 100 + np.cumsum(np.random.randn(100))\n",
        "data = pd.DataFrame({'Date': dates, 'Price': stock_prices})\n",
        "prices = data['Price'].to_numpy()  # Get numpy array of prices\n"
      ],
      "metadata": {
        "id": "1lEOnU5bkB0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas - Applying (Level 3) Scenario: You have a dataset of student grades in different subjects. You need to calculate each student's average grade and assign letter grades based on the average."
      ],
      "metadata": {
        "id": "XUOc2eGXkEUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = {'StudentID': [1, 2, 3, 4, 5],\n",
        "        'Math': [85, 92, 78, 88, 95],\n",
        "        'Science': [90, 88, 85, 92, 80],\n",
        "        'English': [75, 80, 92, 85, 90]}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "SW0oKXRLkHBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas - Analyzing (Level 4) Scenario: You are analyzing sales data for different product categories. You need to identify the best-selling categories and analyze their sales trends over time."
      ],
      "metadata": {
        "id": "opQtQnmtkKkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range('2023-01-01', periods=90)  # 3 months of data\n",
        "categories = ['Electronics', 'Clothing', 'Books']\n",
        "sales = {cat: 100 + np.cumsum(np.random.randint(-10, 20, size=90)) for cat in categories}\n",
        "df = pd.DataFrame(sales, index=dates)\n",
        "df = df.resample('W').sum()  # Aggregate weekly sales\n"
      ],
      "metadata": {
        "id": "waw3rLHHkRfb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Scenario: You have collected data on air quality from different sources, including government agencies, research institutions, and citizen science initiatives. Dataset: No specific dataset is provided. The focus is on analyzing the data sources. Question: Analyze the challenges of combining air quality data from these diverse sources. Discuss potential issues related to data accuracy, consistency, measurement methods, and spatial and temporal coverage. How would you validate and harmonize the data before using it for analysis or modeling? What are the ethical considerations involved in using citizen science data for environmental monitoring?"
      ],
      "metadata": {
        "id": "a_XCXu5TkaJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create synthetic datasets for three different sources\n",
        "np.random.seed(42)\n",
        "\n",
        "# Government data (accurate and consistent)\n",
        "gov_data = pd.DataFrame({\n",
        "    'Date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
        "    'PM2.5': np.random.normal(50, 10, 10)  # in µg/m³\n",
        "})\n",
        "\n",
        "# Research institution data (accurate but with some missing values)\n",
        "research_data = pd.DataFrame({\n",
        "    'Date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
        "    'PM2.5': np.random.normal(52, 8, 10)  # in µg/m³\n",
        "})\n",
        "research_data.loc[3, 'PM2.5'] = np.nan  # Introducing missing value\n",
        "\n",
        "# Citizen science data (lower accuracy, might have some bias)\n",
        "citizen_data = pd.DataFrame({\n",
        "    'Date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
        "    'PM2.5': np.random.normal(55, 12, 10)  # in µg/m³\n",
        "})\n",
        "citizen_data['PM2.5'] += 5  # Citizen sensors could have calibration bias\n",
        "\n",
        "# Merging all dataframes (inner join on Date)\n",
        "merged_data = pd.merge(gov_data, research_data, on='Date', suffixes=('_gov', '_research'))\n",
        "merged_data = pd.merge(merged_data, citizen_data, on='Date', suffixes=('', '_citizen'))\n",
        "\n",
        "# Fill missing values using the median of the column\n",
        "merged_data['PM2.5_research'] = merged_data['PM2.5_research'].fillna(merged_data['PM2.5_research'].median())\n",
        "\n",
        "# Standardizing PM2.5 values: Convert all sources to the same unit (µg/m³) (already in the same unit in this example)\n",
        "\n",
        "# Average the PM2.5 readings across sources\n",
        "merged_data['PM2.5_avg'] = merged_data[['PM2.5_gov', 'PM2.5_research', 'PM2.5_citizen']].mean(axis=1)\n",
        "\n",
        "# Displaying the cleaned and harmonized data\n",
        "print(merged_data)\n"
      ],
      "metadata": {
        "id": "zOzlyB70kg4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Code: Data Creation:\n",
        "\n",
        "We create three synthetic datasets representing data from government, research institutions, and citizen science. Each dataset has a Date column and a PM2.5 column (representing air quality), and we assume all data is in the unit of µg/m³. For research data, we introduce a missing value at index 3 for demonstration purposes. Data Merging:\n",
        "\n",
        "We merge the datasets based on the Date column using pd.merge(). This combines all the data from the three sources into one DataFrame. The suffixes argument ensures that columns with the same name (PM2.5) from different sources are labeled appropriately (_gov, _research, _citizen). Handling Missing Data:\n",
        "\n",
        "We handle missing data in the PM2.5_research column using the fillna() method. In this case, we fill the missing value with the median of the available values in that column. Standardizing Data:\n",
        "\n",
        "In this example, all the data is already in the same unit (µg/m³), but in a real scenario, you may need to convert units (e.g., from ppm to µg/m³). This is a critical step when working with multiple data sources. Calculating Average:\n",
        "\n",
        "To get a harmonized measurement of PM2.5 from all sources, we calculate the average of the three columns (PM2.5_gov, PM2.5_research, and PM2.5_citizen) using the .mean(axis=1) method. This averages the values row-wise (for each day). Output:\n",
        "\n",
        "Finally, the cleaned and combined data is printed. This DataFrame includes columns from each data source and an average PM2.5 value."
      ],
      "metadata": {
        "id": "dByG5OnXki07"
      }
    }
  ]
}